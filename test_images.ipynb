{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "import cv2\n",
    "from imutils.video import VideoStream\n",
    "import os\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import img_to_array\n",
    "import imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_image(face_image):\n",
    "    face_image = cv2.resize(face_image, (48,48))\n",
    "    face_image = cv2.cvtColor(face_image, cv2.COLOR_BGR2GRAY)\n",
    "    face_image = face_image.astype(\"float\") / 255.0\n",
    "    face_image= img_to_array(face_image)\n",
    "    face_image = np.expand_dims(face_image, axis=0)\n",
    "    return face_image\n",
    "\n",
    "def prepare_image_tf1(face_image):\n",
    "    face_image = cv2.resize(face_image, (48,48))\n",
    "    face_image = face_image.astype(\"float\") / 255.0\n",
    "    face_image= img_to_array(face_image)\n",
    "    face_image = np.expand_dims(face_image, axis=0)\n",
    "    return face_image\n",
    "\n",
    "def prepare_image_tf2(face_image):\n",
    "    face_image = cv2.resize(face_image, (128,128))\n",
    "    face_image = face_image.astype(\"float\") / 255.0\n",
    "    face_image= img_to_array(face_image)\n",
    "    face_image = np.expand_dims(face_image, axis=0)\n",
    "    return face_image\n",
    "\n",
    "def preprocess_img(face_image,rgb=True,w=48,h=48):\n",
    "    face_image = cv2.resize(face_image, (w,h))\n",
    "    if rgb == False:\n",
    "        face_image = cv2.cvtColor(face_image, cv2.COLOR_BGR2GRAY)\n",
    "    face_image = face_image.astype(\"float\") / 255.0\n",
    "    face_image= img_to_array(face_image)\n",
    "    face_image = np.expand_dims(face_image, axis=0)\n",
    "    return face_image\n",
    "\n",
    "\n",
    "def bounding_box(img,box,match_name):\n",
    "    for i in np.arange(len(box)):\n",
    "        y0,x1,y1,x0 = box[i]\n",
    "        img = cv2.rectangle(img,\n",
    "                      (x0,y0),\n",
    "                      (x1,y1),\n",
    "                      (0,255,0),3);\n",
    "        cv2.putText(img, match_name, (x0, y0-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,255,0), 2)\n",
    "    return img\n",
    "\n",
    "def sel_load_model(model_select):\n",
    "    if model_select == 1:\n",
    "        name_model = 'model_baseline_v1.hdf5'\n",
    "        w,h = (48,48)\n",
    "        rgb = False\n",
    "    elif model_select == 2:\n",
    "        name_model = 'model_v1.hdf5 '\n",
    "        w,h = (48,48)\n",
    "        rgb = False\n",
    "    elif model_select == 3:\n",
    "        name_model = 'model_baseline_dropout.h5'\n",
    "        w,h = (48,48)\n",
    "        rgb = False\n",
    "    elif model_select == 4:\n",
    "        name_model = 'model_dropout.hdf5'\n",
    "        w,h = (48,48)\n",
    "        rgb = False\n",
    "    elif model_select == 5:\n",
    "        name_model = 'model_baseline_tf_learning.h5'\n",
    "        w,h = (48,48)\n",
    "        rgb = True\n",
    "    elif model_select == 6:\n",
    "        name_model = 'model_tf_learning_mobilnet.hdf5'\n",
    "        w,h = (128,128)\n",
    "        rgb = True\n",
    "    elif model_select == 7:\n",
    "        name_model = 'model_tf_learning_ResNet50.hdf5'\n",
    "        w,h = (48,48)\n",
    "        rgb = True\n",
    "    elif model_select == 8:\n",
    "        name_model = 'model_tf_learning_InceptionV3.hdf5'\n",
    "        w,h = (75,75)\n",
    "        rgb = True\n",
    "    elif model_select == 9:\n",
    "        name_model = 'model_tf_learning_VGG16.hdf5'\n",
    "        w,h = (48,48)\n",
    "        rgb = True\n",
    "\n",
    "    name_model = './Modelos/'+name_model  \n",
    "    model = load_model(name_model)\n",
    "    return model,rgb,w,h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "__call__(): incompatible function arguments. The following argument types are supported:\n    1. (self: dlib.fhog_object_detector, image: array, upsample_num_times: int=0) -> dlib.rectangles\n\nInvoked with: <dlib.fhog_object_detector object at 0x7fb60ee71ab0>, None, 1",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-13bd5fd466a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data_test/friends11.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# detectar_rostro\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_recognition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mface_locations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mface_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/face_recognition/api.py\u001b[0m in \u001b[0;36mface_locations\u001b[0;34m(img, number_of_times_to_upsample, model)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_trim_css_to_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_rect_to_css\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mface\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_raw_face_locations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_times_to_upsample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cnn\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_trim_css_to_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_rect_to_css\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mface\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_raw_face_locations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_times_to_upsample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/face_recognition/api.py\u001b[0m in \u001b[0;36m_raw_face_locations\u001b[0;34m(img, number_of_times_to_upsample, model)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcnn_face_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_times_to_upsample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mface_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_times_to_upsample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __call__(): incompatible function arguments. The following argument types are supported:\n    1. (self: dlib.fhog_object_detector, image: array, upsample_num_times: int=0) -> dlib.rectangles\n\nInvoked with: <dlib.fhog_object_detector object at 0x7fb60ee71ab0>, None, 1"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "1. model_baseline_v1\n",
    "2. model_v1\n",
    "3. model_baseline_dropout\n",
    "4. model_dropout\n",
    "5. model_baseline_tf_learning\n",
    "6. model_tf_learning\n",
    "'''\n",
    "sel_model = 4\n",
    "model,rgb,w,h = sel_load_model(sel_model)\n",
    "labels = ['angry','disgust','fear','happy','neutral','sad','surprise']\n",
    "\n",
    "#ingestar data\n",
    "im = cv2.imread('data_test/friends11.jpg')\n",
    "# detectar_rostro\n",
    "box = face_recognition.face_locations(im)\n",
    "x0,y1,x1,y0 = box[0]\n",
    "face_image = im[x0:x1,y0:y1]\n",
    "# preprocesar data\n",
    "'''\n",
    "if sel_model in [1,2,3,4]:\n",
    "    face_image = prepare_image(face_image)\n",
    "elif sel_model == 5:\n",
    "    face_image = prepare_image_tf1(face_image)\n",
    "elif sel_model == 6:\n",
    "    face_image = prepare_image_tf2(face_image)\n",
    "'''\n",
    "face_image = preprocess_img(face_image,rgb,w,h)\n",
    "# predecir imagen\n",
    "prediction = model.predict(face_image)[0]\n",
    "emotion = labels[prediction.argmax()]\n",
    "# visualizacion\n",
    "img_post = bounding_box(im,box,emotion)\n",
    "cv2.imshow('emotion_detection',img_post)\n",
    "cv2.imwrite('P_neutral.jpg', img_post) \n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37664bitocrenvvenv7fc674b1f8e54976842dc743de18c731",
   "display_name": "Python 3.7.6 64-bit ('ocr-env': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}